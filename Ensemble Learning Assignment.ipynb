{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6847ae-33bf-4725-b7c8-36a36de2eada",
   "metadata": {},
   "source": [
    "# Q.6) Write a Python program to:● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72dd6f0c-e5dc-4e2a-8fb2-fa43f522eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf64e52-749f-4386-b31c-46f441eb13b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 5 Important Features:\")\n",
    "print(importance_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe967263-a885-452c-8ddc-2fa8f3bc5053",
   "metadata": {},
   "source": [
    "# Q.7) Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fc7c1f-99d2-471b-89ee-eda20f67eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77320ed7-898f-4c17-9495-5e1ac06e4844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", dt_acc)\n",
    "print(\"Bagging Accuracy:\", bag_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b70f09-3949-438f-b1ec-ec4e3c7e3101",
   "metadata": {},
   "source": [
    "# Q.8) Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a522488a-cdbe-44bc-9c13-5a2be0c8663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec7a18f-e770-4d13-80c7-7e755f23d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'n_estimators': 50}\n",
      "Final Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Final Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18ab1c-a74d-44d4-820a-446e83f0090c",
   "metadata": {},
   "source": [
    "# Q.9) Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771819fa-3a11-404d-a6e7-821eaf5bf836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1ceae3-272f-41f8-b165-f4f8b8b1652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging MSE: 0.27872278915343157\n",
      "Random Forest MSE: 0.25424371393528344\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "bag = BaggingRegressor(random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "\n",
    "print(\"Bagging MSE:\", bag_mse)\n",
    "print(\"Random Forest MSE:\", rf_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd571bf-ed79-4b67-ac80-f71cba98815f",
   "metadata": {},
   "source": [
    "# Q.10) You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9718f4d2-18fc-4ac8-8519-cb652d99b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a18a012-7b46-4788-86c8-6d18ef01ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['age', 'income', 'loan_amount', 'credit_score', 'employment_years', 'transaction_count', 'avg_transaction_amount', 'missed_payments', 'loan_default']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "print(\"Dataset Columns:\", list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23dab36-038b-4ead-87d8-0027eb3521b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"loan_default\"\n",
    "\n",
    "if target_col not in data.columns:\n",
    "    raise ValueError(f\"Target column '{target_col}' not found. Available columns: {list(data.columns)}\")\n",
    "\n",
    "X = data.drop(target_col, axis=1)\n",
    "y = data[target_col]\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977db8eb-d766-4112-b321-c405b6bec6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461523a8-7f55-4379-8d0c-bb59875e2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42)\n",
    "boosting_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1b90cc-0fe6-40a6-b1f9-6fbeceb14ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging CV AUC: 0.4486\n",
      "Boosting CV AUC: 0.4568\n"
     ]
    }
   ],
   "source": [
    "cv_bagging = cross_val_score(bagging_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "cv_boosting = cross_val_score(boosting_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Bagging CV AUC: {cv_bagging.mean():.4f}\")\n",
    "print(f\"Boosting CV AUC: {cv_boosting.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf63f9a-265e-446d-84f3-bc3b1ad37fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       159\n",
      "           1       0.18      0.05      0.08        41\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.49      0.50      0.47       200\n",
      "weighted avg       0.67      0.76      0.70       200\n",
      "\n",
      "Test AUC: 0.4919\n"
     ]
    }
   ],
   "source": [
    "# Select Best Model & Evaluate\n",
    "final_model = boosting_model if cv_boosting.mean() > cv_bagging.mean() else bagging_model\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Test AUC: {roc_auc_score(y_test, y_pred_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690e3cd7-abef-4871-befa-10cc49408ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnsemble learning combines multiple models to achieve better predictive performance\\nthan individual models. In predicting loan defaults:\\n\\n1. Reduces the risk of relying on a single weak model.\\n2. Captures complex patterns in both demographic and transaction history data.\\n3. Improves robustness to noise and outliers.\\n4. Boosting helps focus on difficult-to-predict customers.\\n5. Bagging improves stability and reduces overfitting, especially with high variance data.\\n\\nThis translates into fewer false approvals and missed defaulters, directly\\nimpacting financial risk management.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble learning combines multiple models to achieve better predictive performance\n",
    "than individual models. In predicting loan defaults:\n",
    "\n",
    "1. Reduces the risk of relying on a single weak model.\n",
    "2. Captures complex patterns in both demographic and transaction history data.\n",
    "3. Improves robustness to noise and outliers.\n",
    "4. Boosting helps focus on difficult-to-predict customers.\n",
    "5. Bagging improves stability and reduces overfitting, especially with high variance data.\n",
    "\n",
    "This translates into fewer false approvals and missed defaulters, directly\n",
    "impacting financial risk management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da307a4-b0e0-49a2-9dd6-728663427d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
